# ===================================
# Backend API Configuration
# ===================================

# Firecrawl API (Required for web scraping)
FIRECRAWL_KEY="YOUR_KEY"
# If you want to use your self-hosted Firecrawl, add the following below:
# FIRECRAWL_BASE_URL="http://localhost:3002"
# FIRECRAWL_CONCURRENCY="2"

# ===================================
# LLM Provider Configuration (NEW - Recommended)
# ===================================
# Choose your LLM provider and model with these simple settings:

# Provider: openai | fireworks | openrouter | custom
LLM_PROVIDER="openai"

# Model name (provider-specific)
LLM_MODEL="o3-mini"

# Optional: Custom endpoint (overrides default for the provider)
# LLM_ENDPOINT="https://api.openai.com/v1"

# API Keys (set the one matching your LLM_PROVIDER)
OPENAI_KEY="YOUR_OPENAI_KEY"
# FIREWORKS_KEY="YOUR_FIREWORKS_KEY"
# OPENROUTER_KEY="YOUR_OPENROUTER_KEY"

# ===================================
# LLM Configuration Examples
# ===================================

# Example 1: OpenAI with o3-mini (default)
# LLM_PROVIDER="openai"
# LLM_MODEL="o3-mini"
# OPENAI_KEY="sk-..."

# Example 2: OpenAI with GPT-4o
# LLM_PROVIDER="openai"
# LLM_MODEL="gpt-4o"
# OPENAI_KEY="sk-..."

# Example 3: Fireworks with DeepSeek R1
# LLM_PROVIDER="fireworks"
# LLM_MODEL="accounts/fireworks/models/deepseek-r1"
# FIREWORKS_KEY="fw_..."

# Example 4: OpenRouter with GPT-4o
# LLM_PROVIDER="openrouter"
# LLM_MODEL="openai/gpt-4o"
# OPENROUTER_KEY="sk-or-..."

# Example 5: OpenRouter with Gemini 2.0 Flash (free)
# LLM_PROVIDER="openrouter"
# LLM_MODEL="google/gemini-2.0-flash-exp:free"
# OPENROUTER_KEY="sk-or-..."

# Example 6: OpenRouter with Claude 3.5 Sonnet
# LLM_PROVIDER="openrouter"
# LLM_MODEL="anthropic/claude-3.5-sonnet"
# OPENROUTER_KEY="sk-or-..."

# Example 7: Local LLM (Ollama, LM Studio, etc.)
# LLM_PROVIDER="custom"
# LLM_MODEL="llama3.1"
# LLM_ENDPOINT="http://localhost:11434/v1"
# OPENAI_KEY="not-needed"

# ===================================
# Legacy Configuration (Backward Compatible)
# ===================================
# These still work but are deprecated. Use LLM_PROVIDER instead.

# OPENAI_KEY="YOUR_KEY"
# OPENAI_ENDPOINT="http://localhost:11434/v1"
# CUSTOM_MODEL="llama3.1"
# FIREWORKS_KEY="YOUR_KEY"

# ===================================
# Other Settings
# ===================================

CONTEXT_SIZE="128000"

# Backend Server Configuration
CONCURRENCY_LIMIT=2
PORT=3051
FRONTEND_URL=http://localhost:3000

# ===================================
# Frontend Configuration
# ===================================

# API URL for frontend to connect to backend
NEXT_PUBLIC_API_URL=http://localhost:3051
NEXT_PUBLIC_APP_NAME=Deep Research
